# Project 1: The Adaptive Content Engine

> **Focus**: Mastering LangChain Expression Language (LCEL), Tool Calling, and Dynamic Routing.

A professional marketing pipeline that **researches**, **writes**, and **validates** content â€” not just a simple writer.

---

## System Architecture

```mermaid
graph LR
    subgraph "Adaptive Content Engine Pipeline"
        A["ğŸ” Search Node<br/>(Tavily API)"] --> B["ğŸ“Š Synthesis Node<br/>(LCEL Chain)"]
        B --> C["âœï¸ Writer Node<br/>(Brand Voice LLM)"]
        C --> D{"ğŸ›¡ï¸ Validator Node<br/>(Critic LLM)"}
        D -->|"âœ… PASS"| E["ğŸ“„ Final Output"]
        D -->|"âŒ FAIL (max 1 retry)"| C
    end

    T["ğŸ“¥ Topic Input"] --> A
    E --> L["ğŸ“ˆ LangSmith Traces"]

    style A fill:#1e3a5f,stroke:#4a90d9,color:#fff
    style B fill:#2d4a1e,stroke:#6abf4b,color:#fff
    style C fill:#5a1e5f,stroke:#b04ad9,color:#fff
    style D fill:#5f3a1e,stroke:#d9904a,color:#fff
    style E fill:#1e5f5a,stroke:#4ad9b0,color:#fff
    style T fill:#333,stroke:#888,color:#fff
    style L fill:#333,stroke:#888,color:#fff
```

---

## State Management â€” The Data Contract

Every node reads from and writes to a shared `ContentState`. This `TypedDict` is the **single source of truth**.

```python
from typing import TypedDict, List, Optional

class SearchResult(TypedDict):
    url: str
    title: str
    content: str

class ContentState(TypedDict):
    # --- Input ---
    topic: str                            # User's topic
    brand_voice: str                      # e.g., "professional", "casual", "technical"

    # --- Search Phase ---
    search_results: List[SearchResult]    # Raw Tavily results (5 sources)

    # --- Synthesis Phase ---
    synthesized_context: str              # Structured summary of search results

    # --- Writing Phase ---
    draft_content: str                    # Generated article/content

    # --- Validation Phase ---
    validation_passed: bool               # Did the critic approve?
    validation_feedback: str              # Critic's feedback if failed
    retry_count: int                      # Track retries (max 1)

    # --- Final ---
    final_content: str                    # Approved content
```

### How Data Flows

```
Topic + Brand Voice
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Search Node    â”‚â”€â”€â–º Populates: search_results
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Synthesis Node   â”‚â”€â”€â–º Reads: search_results
â”‚   (LCEL Chain)   â”‚â”€â”€â–º Populates: synthesized_context
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Writer Node     â”‚â”€â”€â–º Reads: synthesized_context, brand_voice, validation_feedback
â”‚ (Brand Voice)    â”‚â”€â”€â–º Populates: draft_content
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Validator Node   â”‚â”€â”€â–º Reads: draft_content, search_results
â”‚  (Critic LLM)   â”‚â”€â”€â–º Populates: validation_passed, validation_feedback
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€ PASS â”€â”€â–º final_content = draft_content
       â””â”€â”€ FAIL â”€â”€â–º retry_count++ â†’ back to Writer (max 1 retry)
```

---

## Key LCEL Concepts

| Concept | Where Used | What It Does |
|---------|-----------|--------------|
| `RunnablePassthrough` | Synthesis & Writer chains | Passes input data through unchanged while adding new computed fields |
| `itemgetter` | Between nodes | Extracts specific keys from the state dict to feed into the next chain |
| `RunnableLambda` | Search Node | Wraps the Tavily API call as a Runnable |
| `prompt \| llm \| parser` | All LLM nodes | The core LCEL pipe pattern |

### Example: The Pipe Pattern

```python
from operator import itemgetter
from langchain_core.runnables import RunnablePassthrough

# This is how LCEL chains work:
synthesis_chain = (
    {
        "search_results": itemgetter("search_results"),  # Extract specific keys
        "topic": itemgetter("topic")
    }
    | synthesis_prompt        # ChatPromptTemplate
    | llm                    # ChatOpenAI
    | StrOutputParser()      # Parse LLM output to string
)
```

---

## Sprint Plan

### Sprint 0 â€” Project Scaffolding
- Project structure, `requirements.txt`, `.env` setup
- `ContentState` TypedDict definition
- LangSmith tracing configuration

### Sprint 1 â€” Search Node
- Tavily API integration
- Fetch 5 credible sources for a given topic
- Populate `search_results` in state

### Sprint 2 â€” Synthesis & Writer Nodes
- LCEL chain for summarizing search results
- Brand voice writer with retry-aware prompting
- `RunnablePassthrough` + `itemgetter` piping

### Sprint 3 â€” Validator Node + Self-Correction Loop
- Critic LLM with Pydantic `ValidationResult` structured output
- Hallucination check against original search results
- Conditional loop-back (max 1 retry)

### Sprint 4 â€” Integration & Polish
- Full pipeline assembly
- LangSmith trace verification
- End-to-end testing

---

## Project Structure

```
project1_content_engine/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ state.py              # ContentState TypedDict + Pydantic models
â”œâ”€â”€ nodes/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ search.py         # Tavily search node
â”‚   â”œâ”€â”€ synthesis.py      # LCEL synthesis chain
â”‚   â”œâ”€â”€ writer.py         # Brand voice writer
â”‚   â””â”€â”€ validator.py      # Hallucination critic
â”œâ”€â”€ pipeline.py           # Full pipeline assembly
â””â”€â”€ main.py               # Entry point
```

---

## Industry Standards Used

| Standard | Tool | Purpose |
|----------|------|---------|
| **Tracing** | LangSmith | Full observability of every LLM call, latency, token usage |
| **Output Parsing** | Pydantic v2 | Structured, validated output from the Validator node |
| **Environment Config** | python-dotenv | Secure API key management via `.env` |
| **Search** | Tavily API | Industry-grade web search (not scraping) |

---

## Dependencies

```
langchain>=0.3.0
langchain-openai>=0.3.0
langchain-community>=0.3.0
langsmith>=0.2.0
tavily-python>=0.5.0
pydantic>=2.0
python-dotenv>=1.0.0
```
